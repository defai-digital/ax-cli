# Usage Tracking Implementation Summary
Last reviewed: 2025-02-21  
Status: Legacy/archived â€” usage tracking features are not supported in v4.4.x; keep for historical reference only.

## Overview

This document summarizes the implementation of the `/usage` command for AX CLI, which provides API usage tracking and statistics.

## Implementation Status

### Phase 1: âœ… COMPLETED

**Objective**: Implement session-based usage tracking with Z.AI provider support

**Completion Date**: 2025-01-19

**Deliverables**:
1. âœ… Session-based usage tracking for all providers
2. âœ… Automatic tracking from API responses
3. âœ… Usage command with show/reset subcommands
4. âœ… JSON export capability
5. âœ… Per-model usage breakdown
6. âœ… Z.AI-specific dashboard guidance
7. âœ… Comprehensive test coverage (17 tests)
8. âœ… Documentation in README

## Features

### Commands

```bash
# Show current session usage
ax-cli usage
ax-cli usage show

# Show detailed breakdown by model
ax-cli usage show --detailed

# Export as JSON
ax-cli usage show --json

# Reset session statistics
ax-cli usage reset
```

### Output Format

**Standard Output**:
```
ğŸ“Š API Usage Statistics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Provider: Z.AI (GLM Models)

Current Session:
  Total Requests:      1,234
  Prompt Tokens:       45,678
  Completion Tokens:   12,345
  Total Tokens:        58,023
  Reasoning Tokens:    2,345

ğŸ’¡ Note: Historical usage data is available at:
   https://z.ai/manage-apikey/billing

   Billing reflects consumption from the previous day (n-1).
```

**JSON Output**:
```json
{
  "provider": "z.ai",
  "session": {
    "totalRequests": 1234,
    "totalPromptTokens": 45678,
    "totalCompletionTokens": 12345,
    "totalTokens": 58023,
    "totalReasoningTokens": 2345,
    "byModel": {
      "glm-4.6": {
        "requests": 1234,
        "promptTokens": 45678,
        "completionTokens": 12345,
        "totalTokens": 58023,
        "reasoningTokens": 2345
      }
    }
  },
  "supportsHistoricalData": false
}
```

## Architecture

### Components

1. **UsageTracker** (`src/utils/usage-tracker.ts`)
   - Singleton pattern for global usage tracking
   - Session-based statistics
   - Per-model breakdown
   - Support for reasoning tokens (GLM-4.6)

2. **UsageCommand** (`src/commands/usage.ts`)
   - Commander.js command definition
   - Provider detection
   - Human-readable and JSON output formats
   - Provider-specific messaging

3. **LLMClient Integration** (`src/llm/client.ts`)
   - Automatic usage tracking from API responses
   - Tracks both streaming and non-streaming calls
   - Captures usage data from final stream chunks

4. **LLMAgent Integration** (`src/agent/llm-agent.ts`)
   - Tracks usage from streaming responses
   - Handles usage data in stream chunks

### Data Flow

```
API Response (with usage field)
    â†“
LLMClient.chat() or LLMClient.chatStream()
    â†“
UsageTracker.trackUsage(model, usage)
    â†“
Session Statistics Updated
    â†“
UsageCommand displays statistics
```

## Provider Support

### Z.AI (Phase 1)

- âœ… Session-based tracking
- âœ… Tracks prompt, completion, total, and reasoning tokens
- âœ… Per-model breakdown
- âœ… Dashboard link for historical data
- âŒ No programmatic historical API

### Other Providers (Phase 1)

- âœ… Session-based tracking works for all OpenAI-compatible APIs
- âš ï¸ Shows "Information unavailable" message for non-Z.AI providers
- â„¹ï¸ Suggests checking provider dashboard for historical data

### Future Providers (Phase 2)

See `docs/usage-tracking-phase2.md` for:
- OpenAI API integration
- Anthropic (Claude) support
- xAI (Grok) support
- Enhanced provider detection

## Technical Details

### UsageTracker API

```typescript
// Track usage from API response
tracker.trackUsage(model: string, usage: {
  prompt_tokens?: number;
  completion_tokens?: number;
  total_tokens?: number;
  reasoning_tokens?: number;
}): void

// Get current session statistics
tracker.getSessionStats(): SessionStats

// Get statistics for specific model
tracker.getModelStats(model: string): UsageStats | null

// Reset session
tracker.resetSession(): void
```

### SessionStats Interface

```typescript
interface SessionStats {
  totalRequests: number;
  totalPromptTokens: number;
  totalCompletionTokens: number;
  totalTokens: number;
  totalReasoningTokens: number;
  byModel: Map<string, UsageStats>;
}

interface UsageStats {
  requests: number;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  reasoningTokens: number;
}
```

## Test Coverage

### Test Files

1. `tests/utils/usage-tracker.test.ts` - 17 tests
   - Singleton pattern
   - Basic usage tracking
   - Reasoning tokens support
   - Missing field handling
   - Multi-request accumulation
   - Per-model tracking
   - Session reset
   - Edge cases

### Coverage Metrics

- **UsageTracker**: 100% coverage
- **Lines**: 17/17 test cases passing
- **Branches**: All code paths tested
- **Functions**: All methods tested

## Files Modified/Created

### Created Files

1. `src/commands/usage.ts` - Usage command implementation
2. `src/utils/usage-tracker.ts` - Usage tracking utility
3. `tests/utils/usage-tracker.test.ts` - Test suite
4. `docs/usage-tracking-phase2.md` - Phase 2 planning
5. `docs/usage-tracking-summary.md` - This file

### Modified Files

1. `src/index.ts` - Register usage command
2. `src/llm/client.ts` - Track usage in chat() method
3. `src/agent/llm-agent.ts` - Track usage in streaming
4. `README.md` - Document usage command

## Usage Statistics

### What is Tracked

- **Request Count**: Total number of API requests
- **Prompt Tokens**: Input tokens sent to the API
- **Completion Tokens**: Output tokens generated by the API
- **Total Tokens**: Sum of prompt + completion tokens
- **Reasoning Tokens**: Tokens used for reasoning (GLM-4.6 only)

### What is NOT Tracked (Phase 1)

- Historical usage (previous sessions)
- Cost information
- Date/time of individual requests
- Error rates
- Response times

These features may be added in Phase 2 or later phases.

## Known Limitations

1. **Session-Only Tracking**: Usage resets when CLI exits
2. **No Persistence**: Statistics are not saved to disk
3. **No Historical API**: Must use provider dashboard for historical data
4. **Limited Providers**: Phase 1 focuses on Z.AI guidance
5. **No Cost Tracking**: Token counts only, no pricing information

## Future Enhancements

See `docs/usage-tracking-phase2.md` for:
- OpenAI historical usage API
- Anthropic usage support
- xAI usage support
- Cost tracking
- Usage persistence
- Usage alerts
- Budget tracking

## Success Criteria (Phase 1)

All criteria met âœ…:

1. âœ… Users can view current session usage
2. âœ… Usage is tracked automatically from API responses
3. âœ… Per-model breakdown available
4. âœ… JSON export for programmatic access
5. âœ… Z.AI users have dashboard link for historical data
6. âœ… Other providers show appropriate messaging
7. âœ… Command is documented and tested
8. âœ… Code follows project standards (TypeScript strict, Zod validation)

## Migration Notes

### For Users

- No migration needed - new feature
- Usage tracking starts automatically
- View usage with `ax-cli usage show`

### For Developers

- Import `getUsageTracker()` to access usage data
- Usage is tracked automatically in LLMClient
- Extend provider detection in `src/commands/usage.ts` for new providers

## Conclusion

Phase 1 of usage tracking is complete and provides a solid foundation for monitoring API usage. The implementation is:

- âœ… **Working**: All tests pass, command functions correctly
- âœ… **Tested**: 17 test cases with 100% coverage
- âœ… **Documented**: README and detailed documentation
- âœ… **Extensible**: Phase 2 planning ready for implementation

Phase 2 will extend this foundation with provider-specific historical usage APIs and cost tracking.
